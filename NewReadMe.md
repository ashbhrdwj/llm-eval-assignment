llm-eval-assignment — Evaluation pipeline for LLM tutor responses

Overview
--------
This repository implements a reproducible evaluation pipeline for tutoring/assistant responses generated by LLMs. It provides:

- A FastAPI backend for dataset management, job orchestration, and result reporting.
- An RQ-based worker for background evaluation tasks.
- Adapter pattern for LLM-as-judge (mock, Ollama, HuggingFace stubs).
- A collection of pedagogical metrics (clarity, completeness, accuracy, etc.).
- Persistence of job metadata and per-case results in a SQL database (SQLite for local dev, Postgres supported).
- NDJSON audit trail per-job and CSV export for results.
- Streamlit demo UI for quick exploration.

Quickstart (local, dev)
-----------------------
Prerequisites:
- Python 3.10+
- Redis (or run via docker-compose), optional Postgres/MinIO if you want full stack

Recommended: use the provided Docker Compose stack for a full local environment (redis, postgres, minio, backend, worker).

1) Create a virtualenv and install requirements (or use the provided Dockerfile)

Windows (cmd.exe):

```cmd
python -m venv .venv
.\.venv\Scripts\activate.bat
pip install -r requirements.txt
```

Windows (PowerShell):

```powershell
python -m venv .venv
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process -Force  # only if you get a policy error
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

macOS / Linux:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Optional: use Docker Compose to run the full stack (Redis, Postgres, MinIO, backend, worker). From the repo root:

```bash
docker-compose -f infra/docker-compose.yml up --build
```

2) Run services (simple dev without docker-compose):

- Start Redis (or run Redis in Docker)
- Start the worker in a separate terminal (while the venv is active):

```cmd
python backend\scripts\start_rq_worker.py
```

- Start the FastAPI app (development):

```cmd
uvicorn backend.app.main:app --reload --port 8000
```

3) Run a synchronous evaluation (no Redis) for quick testing:

```cmd
python backend\scripts\run_sync_evaluation.py
```

Key files and components
------------------------
- `backend/app/main.py` — FastAPI application entrypoint and router wiring.
- `backend/app/api/` — API routers for datasets, evaluations, engines, and metrics.
  - `evaluations.py` exposes endpoints to create jobs, check job status, list jobs, get per-case results (DB-backed), and download CSV.
- `backend/app/orchestrator/orchestrator.py` — Job creation, splitting into tasks, and enqueuing to Redis/RQ.
- `backend/app/workers/worker.py` — RQ worker task handler; evaluates each case, computes metrics, writes NDJSON audit, and persists per-case rows to DB (`case_results`).
- `backend/app/db/` — SQLAlchemy session and models. By default the app falls back to SQLite (`sqlite:///./local.db`) if no DATABASE_URL is provided.
- `frontend/streamlit_app.py` — Streamlit demo for creating jobs and viewing results.

Database schema highlights
-------------------------
- `datasets` table — uploaded datasets and metadata.
- `jobs` table — one row per evaluation job; `meta` contains processed_count and `summary` (mean_score, percent_failing, completed_at).
- `case_results` table — persisted per-case evaluation rows with fields: tenant_id, job_id, case_id, aggregated_score, scores (JSON), raw (JSON), evaluated_at.

API - summary and examples
--------------------------
All endpoints are prefixed with `/api/v1`.

1) Create evaluation (enqueue)

POST /api/v1/tenants/{tenant_id}/evaluations
Body: JSON matching `EvaluationCreateRequest` (see Pydantic models in `backend/app/api/models.py`).

Response: 202 Accepted with `job_id`.

2) Get job status

GET /api/v1/tenants/{tenant_id}/evaluations/{job_id}

Returns DB-backed job metadata if present (`status`, `num_cases`, and `meta.summary` after completion).

3) List jobs for tenant (DB-backed)

GET /api/v1/tenants/{tenant_id}/evaluations

Returns recent jobs with `meta` included where available.

4) List per-case results (paginated)

GET /api/v1/tenants/{tenant_id}/evaluations/{job_id}/cases?limit=50&offset=0&min_score=0.2

Response: JSON containing `total`, `limit`, `offset`, and `cases` (each case includes `case_id`, `aggregated_score`, `scores`, `engine_id`, `status`, `evaluated_at`).

5) Get a single case result

GET /api/v1/tenants/{tenant_id}/evaluations/{job_id}/cases/{case_id}

Response: full case row including `raw` and `scores` JSON.

6) Download CSV

GET /api/v1/tenants/{tenant_id}/evaluations/{job_id}/results.csv

Downloads a simple CSV with `case_id`, `aggregated_score`, `evaluated_at` (useful for spreadsheets).

Developer notes & extension points
----------------------------------
- Per-case DB storage is best-effort in the worker; the NDJSON file remains the primary audit trail.
- To enable stronger transactional guarantees add retries and a durable job/event store.
- Adapter improvements: connect `backend/app/engines/ollama_adapter.py` and `hf_adapter.py` to real runtimes and add deterministic seeding controls.
- Auth: current dev auth uses `DEV_API_KEY` environment variable. Replace with proper OAuth/JWT for production.
- Pagination: the case list endpoint supports simple offset/limit; you can switch to cursor-based pagination for large result sets.

Testing
-------
Run unit tests and integration tests with pytest:

```powershell
pytest -q
```

If you have created new endpoints (for example the per-case DB-backed endpoints), add tests under `tests/` and run the specific file:

```powershell
pytest tests/test_evaluations.py -q
```

Contributing
------------
- Please open issues or PRs for new metrics, adapters, or stability improvements.

Troubleshooting
---------------
- ``DEV_API_KEY`` not working: set the environment variable before starting the app. Example (Windows cmd):

```cmd
set DEV_API_KEY=your-dev-key
```

Example (PowerShell):

```powershell
$env:DEV_API_KEY = 'your-dev-key'
```

- Database errors when the app starts: by default the app will create tables in the configured DB. For local dev the default is `sqlite:///./local.db`. If you use Postgres via Docker Compose, make sure `DATABASE_URL` points to it (the compose file sets it for you).

- Worker doesn't pick up tasks: ensure Redis is running and `REDIS_URL` is set (or use docker-compose). The worker will log queue activity to the terminal.

- Permissions to run PowerShell activation: if PowerShell blocks script execution, run the `Set-ExecutionPolicy` line in the PowerShell quickstart (process-scope only).

Quick API Examples
------------------
Create an evaluation (curl example):

```bash
curl -X POST http://localhost:8000/api/v1/tenants/demo/evaluations \
  -H "Content-Type: application/json" \
  -d '{"dataset_id":"sample_dataset_v1","engine_selector":{"primary":"mock"},"mode":"async"}'
```

Get job status:

```bash
curl http://localhost:8000/api/v1/tenants/demo/evaluations/<job_id>
```

List cases for a job (paginated):

```bash
curl "http://localhost:8000/api/v1/tenants/demo/evaluations/<job_id>/cases?limit=20&offset=0"
```

Download CSV of results:

```bash
curl -OJ http://localhost:8000/api/v1/tenants/demo/evaluations/<job_id>/results.csv
```

Contact
-------
Repository owner: AmanPawar9

Acknowledgements
----------------
This project was developed as part of a guided implementation for an LLM evaluation assignment.
