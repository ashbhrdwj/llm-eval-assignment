# ğŸ§  LLM Evaluation Pipeline for AI Tutor System

## ğŸ“˜ Overview

This project implements a **modular evaluation pipeline** designed to assess the **quality, accuracy, and pedagogical soundness** of responses generated by an **AI Tutor Orchestrator**.  
The system simulates a real-world educational evaluation setup where the tutor receives student queries and responds with explanations, problem-solving steps, and learning resources.

The goal is to **quantitatively and qualitatively evaluate** these responses using well-defined rubrics and metrics tailored for different **grade levels** and **academic subjects**.

---

## ğŸ•’ Assignment Timeline

| Duration | Breakdown |
|-----------|------------|
| **Total Time:** 6 hours | **Day 1 (3 hrs):** Core pipeline + metrics + dataset<br>**Day 2 (3 hrs):** API integration + batch evaluation + final results |

**Evaluation Weight:**  
- 50% â€“ Code Quality  
- 50% â€“ Evaluation Design  

---

## ğŸ¯ Problem Understanding

Modern educational LLM systems aim to serve as personalized tutors. However, evaluating their **pedagogical quality** remains a major challenge.

This project builds a **systematic evaluation pipeline** that:
1. Measures educational response quality across **multiple dimensions**
2. Adapts evaluation to **grade levels** (Kâ€“College)
3. Differentiates between **subject contexts** (STEM, Humanities, etc.)
4. Provides **actionable metrics** to guide model improvement
5. Is designed for **production scalability** with real API endpoints

---


### ğŸ§© Core Components

| Component | Description |
|------------|-------------|
| **Mock Tutor (Day 1)** | Generates responses from any LLM of your choice. |
| **Evaluator Module** | Applies metrics and LLM-judge scoring. |
| **Results Storage** | Saves detailed JSON/CSV logs of scores and reasoning. |
| **Real Endpoint Client (Day 2)** | Integrates with `/tutor/query` API for actual tutor evaluations. |
| **Batch Evaluator** | Runs evaluation on all test cases, aggregates results. |

---

## ğŸ“Š Evaluation Metrics

The pipeline uses **LLM-as-a-Judge** to evaluate tutor responses along four key pedagogical dimensions.  
Each metric is rated on a **1â€“5 scale**, with thresholds for pass/fail categorization.

| Metric | Description | Example Prompt (to LLM-Judge) |
|---------|--------------|-------------------------------|
| **Clarity** | Is the explanation understandable for the target grade level? | â€œRate how clear the explanation is for a Grade 7 student.â€ |
| **Completeness** | Does the response fully address the question? | â€œDoes the answer include all key ideas expected for this topic?â€ |
| **Accuracy** | Is the content factually correct and aligned with educational standards? | â€œEvaluate factual correctness of this explanation.â€ |
| **Appropriateness** | Is the language and complexity suitable for the studentâ€™s age? | â€œIs this explanation appropriate for an elementary student?â€ |
| **Long-term Memory** | Is the model remembering long conversations? | â€œCan you repeat the answer to my first question?â€ |

### ğŸ§® Scoring Rubric (1â€“5 Scale)

| Score | Meaning |
|-------|----------|
| 1 | Poor: incorrect or confusing |
| 2 | Weak: partial or off-level |
| 3 | Acceptable: adequate clarity and accuracy |
| 4 | Strong: accurate and well-articulated |
| 5 | Excellent: perfectly clear, correct, and grade-appropriate |

---

## ğŸ§‘â€ğŸ« Test Dataset Design

### **Dataset Composition**

The dataset (create your mock data) includes **15â€“20 diverse examples** spanning:
- Grade Levels: Elementary â†’ College  
- Subjects: Math, Science, English, History  
- Query Types: Explanations, Problem-solving, Analysis

### **Example Entry**
```json
{
  "id": "test_003",
  "student_query": "Explain what mitosis is",
  "grade_level": "high_school",
  "subject": "biology",
  "expected_concepts": ["cell division", "chromosomes", "phases"],
  "ground_truth_answer": "Mitosis is the process by which a cell divides to produce two identical daughter cells, ensuring each has the same number of chromosomes."
}

### **Success Indicators**
A strong submission will:

âœ… Run successfully with minimal setup

âœ… Evaluate 15+ diverse test cases

âœ… Produce interpretable scores with reasoning

âœ… Show clear understanding of educational content evaluation

âœ… Demonstrate senior-level code organization

âœ… Document thoughtful trade-offs and design decisions

âœ… Be easily extensible to real endpoints
