{"case_id": "test_001", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 4, "confidence": 0.92, "notes": "3/4 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/4 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=10, grade=middle_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.5625, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.257748Z", "trace_id": "89ee8373-cf6c-4e84-a8fc-64a96fd4c203", "query_type": "other", "failed": false, "raw": {"tutor_response": "What is photosynthesis?\n\nAnswer: This explanation mentions: sunlight, chlorophyll, glucose.", "metrics": {"clarity": {"value": 5, "confidence": 0.97, "notes": "Mocked score for clarity"}, "completeness": {"value": 3, "confidence": 0.63, "notes": "Mocked score for completeness"}, "accuracy": {"value": 5, "confidence": 0.79, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 3, "confidence": 0.93, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 3, "confidence": 0.8, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 5, "confidence": 0.8, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 3, "confidence": 0.85, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_001"}}}
{"case_id": "test_002", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=19, grade=middle_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.65625, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.273687Z", "trace_id": "fb996be0-1b8c-4521-8e05-f13c5bbde273", "query_type": "problem_solving", "failed": false, "raw": {"tutor_response": "Solve 2x + 5 = 15. Show steps.\n\nAnswer: This explanation mentions: isolate variable, subtract 5, divide by 2.", "metrics": {"clarity": {"value": 4, "confidence": 0.73, "notes": "Mocked score for clarity"}, "completeness": {"value": 4, "confidence": 0.71, "notes": "Mocked score for completeness"}, "accuracy": {"value": 5, "confidence": 0.85, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 3, "confidence": 0.62, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 4, "confidence": 0.73, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 3, "confidence": 0.96, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 3, "confidence": 0.79, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_002"}}}
{"case_id": "test_003", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 4, "confidence": 0.88, "notes": "3/5 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/5 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=10, grade=high_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.6499999999999999, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.295785Z", "trace_id": "bcd985f0-5ac3-45fc-9cc4-f3c94e4a3f5a", "query_type": "explanation", "failed": false, "raw": {"tutor_response": "Explain mitosis briefly.\n\nAnswer: This explanation mentions: prophase, metaphase, anaphase.", "metrics": {"clarity": {"value": 5, "confidence": 0.91, "notes": "Mocked score for clarity"}, "completeness": {"value": 3, "confidence": 0.75, "notes": "Mocked score for completeness"}, "accuracy": {"value": 5, "confidence": 0.71, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 3, "confidence": 0.67, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 5, "confidence": 0.75, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 5, "confidence": 0.99, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 5, "confidence": 0.82, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_003"}}}
{"case_id": "test_004", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=15, grade=high_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.59375, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.305393Z", "trace_id": "db14fe82-d56f-4dc0-8c6b-2951e609c966", "query_type": "other", "failed": false, "raw": {"tutor_response": "What caused the American Civil War?\n\nAnswer: This explanation mentions: slavery, states' rights, economic differences.", "metrics": {"clarity": {"value": 4, "confidence": 0.7, "notes": "Mocked score for clarity"}, "completeness": {"value": 5, "confidence": 0.95, "notes": "Mocked score for completeness"}, "accuracy": {"value": 4, "confidence": 0.87, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 4, "confidence": 0.92, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 5, "confidence": 0.98, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.84, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 3, "confidence": 0.77, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_004"}}}
{"case_id": "test_005", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=19, grade=college"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.6937499999999999, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.310474Z", "trace_id": "f86e875e-2b60-40d2-9eab-3cb7baa9b796", "query_type": "explanation", "failed": false, "raw": {"tutor_response": "Summarize the plot of 'Romeo and Juliet' in 3 sentences.\n\nAnswer: This explanation mentions: feuding families, young love, tragedy.", "metrics": {"clarity": {"value": 4, "confidence": 0.63, "notes": "Mocked score for clarity"}, "completeness": {"value": 5, "confidence": 0.77, "notes": "Mocked score for completeness"}, "accuracy": {"value": 3, "confidence": 0.72, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 3, "confidence": 0.61, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 3, "confidence": 0.75, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.98, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 4, "confidence": 0.79, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_005"}}}
{"case_id": "test_006", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=17, grade=high_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.6937499999999999, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.321203Z", "trace_id": "40a146c8-1375-4d7c-8488-bae37d9941bb", "query_type": "explanation", "failed": false, "raw": {"tutor_response": "Explain why water expands when it freezes.\n\nAnswer: This explanation mentions: hydrogen bonds, lattice structure, lower density.", "metrics": {"clarity": {"value": 3, "confidence": 0.77, "notes": "Mocked score for clarity"}, "completeness": {"value": 4, "confidence": 0.99, "notes": "Mocked score for completeness"}, "accuracy": {"value": 4, "confidence": 0.62, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 4, "confidence": 0.73, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 4, "confidence": 0.79, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 5, "confidence": 0.93, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 5, "confidence": 0.79, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_006"}}}
{"case_id": "test_007", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=13, grade=high_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.59375, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.321203Z", "trace_id": "b4d89a8a-cd65-49a0-8d59-175fc4823a9a", "query_type": "other", "failed": false, "raw": {"tutor_response": "Find derivative of f(x)=3x^2+2x.\n\nAnswer: This explanation mentions: power rule, 2*3x, result 6x+2.", "metrics": {"clarity": {"value": 4, "confidence": 0.8, "notes": "Mocked score for clarity"}, "completeness": {"value": 4, "confidence": 0.97, "notes": "Mocked score for completeness"}, "accuracy": {"value": 5, "confidence": 0.66, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 5, "confidence": 0.94, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 3, "confidence": 0.64, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.89, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 5, "confidence": 0.96, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_007"}}}
{"case_id": "test_008", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 4, "confidence": 0.92, "notes": "3/4 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/4 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=24, grade=college"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 4, "confidence": 0.6, "notes": "some scaffolding indicators found: 1"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.625, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.342203Z", "trace_id": "49c2b33d-ea88-4737-a595-875e9a2ba633", "query_type": "other", "failed": false, "raw": {"tutor_response": "Give a step-by-step plan to debug a Python function that raises a TypeError.\n\nAnswer: This explanation mentions: inspect stack trace, check types, add asserts.", "metrics": {"clarity": {"value": 5, "confidence": 0.85, "notes": "Mocked score for clarity"}, "completeness": {"value": 5, "confidence": 0.83, "notes": "Mocked score for completeness"}, "accuracy": {"value": 5, "confidence": 0.95, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 3, "confidence": 0.87, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 3, "confidence": 0.98, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.81, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 3, "confidence": 0.65, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_008"}}}
{"case_id": "test_009", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "2 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "2/2 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "2/2 expected concepts matched"}, "appropriateness": {"value": 3, "confidence": 0.7, "notes": "word_count=19, grade=elementary"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.5625, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.352780Z", "trace_id": "e5ddb2ce-44a2-4ce2-b00c-a206f83e84fa", "query_type": "other", "failed": false, "raw": {"tutor_response": "Translate 'Good morning' to Spanish and provide a polite follow-up question.\n\nAnswer: This explanation mentions: Buenos d\u00edas, \u00bfC\u00f3mo est\u00e1s?.", "metrics": {"clarity": {"value": 5, "confidence": 0.9, "notes": "Mocked score for clarity"}, "completeness": {"value": 4, "confidence": 0.78, "notes": "Mocked score for completeness"}, "accuracy": {"value": 4, "confidence": 0.61, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 5, "confidence": 0.83, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 3, "confidence": 0.61, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 5, "confidence": 0.78, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 5, "confidence": 0.77, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_009"}}}
{"case_id": "test_010", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=16, grade=college"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.6937499999999999, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.362258Z", "trace_id": "d9cc488d-7363-4529-9934-1da854ec0c66", "query_type": "explanation", "failed": false, "raw": {"tutor_response": "Explain the concept of supply and demand.\n\nAnswer: This explanation mentions: price signals, equilibrium, market forces.", "metrics": {"clarity": {"value": 3, "confidence": 0.7, "notes": "Mocked score for clarity"}, "completeness": {"value": 4, "confidence": 0.72, "notes": "Mocked score for completeness"}, "accuracy": {"value": 4, "confidence": 0.7, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 4, "confidence": 0.66, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 4, "confidence": 0.99, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.71, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 5, "confidence": 0.96, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_010"}}}
{"case_id": "test_011", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=21, grade=high_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.59375, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.368453Z", "trace_id": "867b6ed2-9a2a-4e01-9071-7495057ebed8", "query_type": "other", "failed": false, "raw": {"tutor_response": "What is the main idea of a paragraph that starts with 'In 1914...'?\n\nAnswer: This explanation mentions: WWI context, timeline, causes.", "metrics": {"clarity": {"value": 3, "confidence": 0.84, "notes": "Mocked score for clarity"}, "completeness": {"value": 3, "confidence": 0.65, "notes": "Mocked score for completeness"}, "accuracy": {"value": 4, "confidence": 0.66, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 4, "confidence": 0.74, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 3, "confidence": 0.92, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.6, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 5, "confidence": 0.64, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_011"}}}
{"case_id": "test_012", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 3, "confidence": 0.7, "notes": "word_count=21, grade=elementary"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.6624999999999999, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.368453Z", "trace_id": "aa5a0805-631d-43c3-821e-52f54a57aef8", "query_type": "explanation", "failed": false, "raw": {"tutor_response": "Explain photosynthesis to a 5th grader using a short metaphor.\n\nAnswer: This explanation mentions: sun energy, food for plant, simple metaphor.", "metrics": {"clarity": {"value": 5, "confidence": 0.77, "notes": "Mocked score for clarity"}, "completeness": {"value": 5, "confidence": 0.82, "notes": "Mocked score for completeness"}, "accuracy": {"value": 5, "confidence": 0.63, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 5, "confidence": 0.73, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 4, "confidence": 0.7, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.94, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 5, "confidence": 0.72, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_012"}}}
{"case_id": "test_013", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=26, grade=middle_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 4, "confidence": 0.6, "notes": "some scaffolding indicators found: 1"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.65625, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.384135Z", "trace_id": "b7df57a5-c49f-42af-8a78-fc031c9ae21b", "query_type": "other", "failed": false, "raw": {"tutor_response": "A student asks: 'How do I factor x^2 - 5x + 6?' Provide steps and checks.\n\nAnswer: This explanation mentions: find roots, (x-2)(x-3), check by expansion.", "metrics": {"clarity": {"value": 5, "confidence": 0.69, "notes": "Mocked score for clarity"}, "completeness": {"value": 5, "confidence": 0.76, "notes": "Mocked score for completeness"}, "accuracy": {"value": 4, "confidence": 0.88, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 3, "confidence": 0.99, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 5, "confidence": 0.64, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 4, "confidence": 0.91, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 4, "confidence": 0.89, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_013"}}}
{"case_id": "test_014", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 4, "confidence": 0.92, "notes": "3/4 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/4 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=24, grade=college"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 4, "confidence": 0.8, "notes": "Code present; not deeply analyzed"}, "pedagogy_alignment": {"value": 4, "confidence": 0.6, "notes": "some scaffolding indicators found: 1"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.65625, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.384135Z", "trace_id": "2498a4f8-94a7-4846-802f-2ccf1640ed0a", "query_type": "other", "failed": false, "raw": {"tutor_response": "Assess this short code: def add(a,b): return a+b \u2014 what tests would you write?\n\nAnswer: This explanation mentions: unit tests, edge cases, type checks.", "metrics": {"clarity": {"value": 4, "confidence": 0.95, "notes": "Mocked score for clarity"}, "completeness": {"value": 5, "confidence": 0.65, "notes": "Mocked score for completeness"}, "accuracy": {"value": 4, "confidence": 0.73, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 4, "confidence": 0.93, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 3, "confidence": 0.7, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 3, "confidence": 0.95, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 4, "confidence": 0.61, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_014"}}}
{"case_id": "test_015", "job_id": "local-sync", "engine_id": "mock/0.1", "scores": {"clarity": {"value": 4, "confidence": 0.8, "notes": "3 expected concepts found"}, "completeness": {"value": 5, "confidence": 1.0, "notes": "3/3 expected concepts present"}, "accuracy": {"value": 4, "confidence": 0.6, "notes": "3/3 expected concepts matched"}, "appropriateness": {"value": 4, "confidence": 0.7, "notes": "word_count=21, grade=middle_school"}, "long_term_memory": {"value": 3, "confidence": 0.4, "notes": "no prior context in case"}, "code_quality": {"value": 3, "confidence": 0.5, "notes": "Not applicable / no code"}, "pedagogy_alignment": {"value": 2, "confidence": 0.4, "notes": "no scaffolding indicators found"}, "multimodal_appropriateness": {"value": 2, "confidence": 0.6, "notes": "no suggested media"}}, "aggregated_score": 0.6937499999999999, "status": "evaluated", "evaluated_at": "2025-11-13T17:35:26.394391Z", "trace_id": "6df537a2-a7fc-4014-bd53-30c3c5e13536", "query_type": "explanation", "failed": false, "raw": {"tutor_response": "Summarize the greenhouse effect in three simple points for grade 8.\n\nAnswer: This explanation mentions: energy trapping, greenhouse gases, temperature rise.", "metrics": {"clarity": {"value": 4, "confidence": 0.99, "notes": "Mocked score for clarity"}, "completeness": {"value": 5, "confidence": 0.98, "notes": "Mocked score for completeness"}, "accuracy": {"value": 3, "confidence": 0.96, "notes": "Mocked score for accuracy"}, "appropriateness": {"value": 5, "confidence": 0.95, "notes": "Mocked score for appropriateness"}, "long_term_memory": {"value": 5, "confidence": 0.66, "notes": "Mocked score for long_term_memory"}, "code_quality": {"value": 5, "confidence": 0.84, "notes": "Mocked score for code_quality"}, "pedagogy_alignment": {"value": 4, "confidence": 0.93, "notes": "Mocked score for pedagogy_alignment"}}, "meta": {"seed": 42, "case_id": "test_015"}}}
